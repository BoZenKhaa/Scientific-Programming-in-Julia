<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Lecture · Scientific Programming in Julia</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://JuliaTeachingCTU.github.io/Scientific-Programming-in-Julia/lecture_05/lecture/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Scientific Programming in Julia logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Scientific Programming in Julia logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Scientific Programming in Julia</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../installation/">Installation</a></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">1: Introduction</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/motivation/">Motivation</a></li><li><a class="tocitem" href="../../lecture_01/basics/">Basics</a></li><li><a class="tocitem" href="../../lecture_01/demo/">Examples</a></li><li><a class="tocitem" href="../../lecture_01/outline/">Outline</a></li><li><a class="tocitem" href="../../lecture_01/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_01/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">2: The power of Type System &amp; multiple dispatch</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_02/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_02/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">3: Design patterns</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_03/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_03/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">4: Packages development, Unit Tests &amp; CI</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_04/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_04/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox" checked/><label class="tocitem" for="menuitem-7"><span class="docs-label">5: Benchmarking, profiling, and performance gotchas</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href>Lecture</a><ul class="internal"><li><a class="tocitem" href="#Numpy-10x-faster-than-julia-what-am-i-doing-wrong?-(solved-julia-faster-now)"><span>Numpy 10x faster than julia what am i doing wrong? (solved julia faster now)</span></a></li><li><a class="tocitem" href="#Detour:-Julia&#39;s-built-in-profiler"><span>Detour: Julia&#39;s built-in profiler</span></a></li><li><a class="tocitem" href="#continueing-with-the-example-Numpy-10x..."><span>continueing with the example Numpy 10x...</span></a></li><li><a class="tocitem" href="#Detour-no-2.-Benchmarking"><span>Detour no 2. Benchmarking</span></a></li><li><a class="tocitem" href="#Coming-back-from-detour"><span>Coming back from detour</span></a></li><li class="toplevel"><a class="tocitem" href="#the-closure-assigned-to-f-make-the-variable-r-captured"><span>the closure assigned to <code>f</code> make the variable <code>r</code> captured</span></a></li><li><a class="tocitem" href="#Using-named-tuple-instead-of-dict"><span>Using named tuple instead of dict</span></a></li><li><a class="tocitem" href="#Performance-of-captured-variable"><span>Performance of captured variable</span></a></li><li><a class="tocitem" href="#Don&#39;t-use-IO-unless-you-have-to"><span>Don&#39;t use IO unless you have to</span></a></li></ul></li><li><a class="tocitem" href="../lab/">Lab</a></li><li><a class="tocitem" href="../hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-8" type="checkbox"/><label class="tocitem" for="menuitem-8"><span class="docs-label">6: Language introspection</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_06/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_06/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_06/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">7: Macros</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_07/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_07/hw/">Homework</a></li></ul></li><li><span class="tocitem">8: Introduction to automatic differentiation</span></li><li><span class="tocitem">9: Manipulating intermediate representation</span></li><li><span class="tocitem">10: Different levels of parallel programming</span></li><li><span class="tocitem">11: Julia for GPU programming</span></li><li><span class="tocitem">12: Uncertainty propagation in ODE</span></li><li><span class="tocitem">13: Learning ODE from data</span></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">5: Benchmarking, profiling, and performance gotchas</a></li><li class="is-active"><a href>Lecture</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Lecture</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaTeachingCTU/Scientific-Programming-in-Julia/blob/master/docs/src/lecture_05/lecture.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="perf_lecture"><a class="docs-heading-anchor" href="#perf_lecture">Benchmarking, profiling, and performance gotchas</a><a id="perf_lecture-1"></a><a class="docs-heading-anchor-permalink" href="#perf_lecture" title="Permalink"></a></h1><p>This class is a short introduction to writing a performant code. As such, we want to cover</p><ul><li>how to identify weak spots in the code</li><li>how to properly benchmark</li><li>common performance anti-patterns</li><li>Julia&#39;s &quot;performance gotchas&quot;, by which we mean performance problems specific for Julia (typicall caused by the lack of understanding of Julia or by a errors in coversion from script to functions)</li></ul><p>Though recall the most important rule of thumb: <strong>Never optimize code from the very beggining.</strong> A much more productive workflow is </p><ol><li>write the code that is idiomatic and easy to understand</li><li>cover the code with unit test, such that you know that the optimized code works the same as the original</li><li>optimize the code</li></ol><p>Premature optimization frequently backfire, because of:</p><ul><li>you might end-up optimizing wrong thing, i.e. you will not optimize performance bottleneck, but something very different</li><li>optimized code can be difficult to read and reason about, which means it is more difficult to make it right.</li></ul><p>It frequently happens that Julia newbies asks on forum that their code in Julia is slow in comparison to the same code in Python (numpy). Most of the time, they make trivial mistakes and it is very educative to go over their mistakes</p><h2 id="Numpy-10x-faster-than-julia-what-am-i-doing-wrong?-(solved-julia-faster-now)"><a class="docs-heading-anchor" href="#Numpy-10x-faster-than-julia-what-am-i-doing-wrong?-(solved-julia-faster-now)">Numpy 10x faster than julia what am i doing wrong? (solved julia faster now)</a><a id="Numpy-10x-faster-than-julia-what-am-i-doing-wrong?-(solved-julia-faster-now)-1"></a><a class="docs-heading-anchor-permalink" href="#Numpy-10x-faster-than-julia-what-am-i-doing-wrong?-(solved-julia-faster-now)" title="Permalink"></a></h2><p><a href="https://discourse.julialang.org/t/numpy-10x-faster-than-julia-what-am-i-doing-wrong-solved-julia-faster-now/29922 ">adapted from</a></p><pre><code class="language-julia hljs">function f(p)														# line 1 
    t0,t1 = p 														# line 2
    m0 = [[cos(t0) - 1im*sin(t0)  0]; [0  cos(t0) + 1im*sin(t0)]]	# line 3
    m1 = [[cos(t1) - 1im*sin(t1)  0]; [0  cos(t1) + 1im*sin(t1)]]	# line 4
    r = m1*m0*[1. ; 0.] 											# line 5
    return abs(r[1])^2 												# line 6
end

function g(p,n)
    return [f(p[:,i]) for i=1:n]
end

g(rand(2,3),3)  # call to force jit compilation

n = 10^6
p = 2*pi*rand(2,n)

@elapsed g(p,n)
</code></pre><p>The first thing we do is to run Profiler, to identify, where the function spends most of the time.</p><h2 id="Detour:-Julia&#39;s-built-in-profiler"><a class="docs-heading-anchor" href="#Detour:-Julia&#39;s-built-in-profiler">Detour: Julia&#39;s built-in profiler</a><a id="Detour:-Julia&#39;s-built-in-profiler-1"></a><a class="docs-heading-anchor-permalink" href="#Detour:-Julia&#39;s-built-in-profiler" title="Permalink"></a></h2><p>Julia&#39;s built-in profiler is part of the standard library in the <code>Profile</code> module implementing a fairly standard sampling based profiler. It a nutshell it asks at regular intervals, where the code execution is currently and marks it and collects this information in some statistics. This allows us to analyze, where these &quot;probes&quot; have occured most of the time which implies those parts are those, where the execution of your function spends most of the time. As such, the profiler has two &quot;controls&quot;, which is the <code>delay</code> between two consecutive probes and the maximum number of probes <code>n</code> (if the profile code takes a long time, you might need to increase it).</p><pre><code class="nohighlight hljs">using Profile
Profile.init(; n = 989680, delay = 0.001))
@profile g(p,n)
Profile.clear()
@profile g(p,n)</code></pre><h3 id="Making-sense-of-profiler&#39;s-output"><a class="docs-heading-anchor" href="#Making-sense-of-profiler&#39;s-output">Making sense of profiler&#39;s output</a><a id="Making-sense-of-profiler&#39;s-output-1"></a><a class="docs-heading-anchor-permalink" href="#Making-sense-of-profiler&#39;s-output" title="Permalink"></a></h3><p>The default <code>Profile.print</code> function shows the call-tree with count, how many times the probe occured in each function sorted from the most to least. The output is a little bit difficult to read and orrient in, therefore there are some visualization options.</p><p>What are our options?</p><ul><li><code>ProfileView</code> is the workhorse with a GTK based api and therefore recommended for those with working GTK</li><li><code>ProfileSVG</code> is the <code>ProfileView</code> with the output exported in SVG format, which is viewed by most browser (it is also very convenient for sharing with others)</li><li><code>PProf.jl</code> is a front-end to Google&#39;s PProf profile viewer https://github.com/JuliaPerf/PProf.jl</li><li><code>StatProfilerHTML</code>  https://github.com/tkluck/StatProfilerHTML.jl</li></ul><p>By personal opinion I mostly use ProfileView (or ProfileSVG) as it indicates places of potential type instability, which as will be seen later is very useful feature. </p><h3 id="Profiling-caveats"><a class="docs-heading-anchor" href="#Profiling-caveats">Profiling caveats</a><a id="Profiling-caveats-1"></a><a class="docs-heading-anchor-permalink" href="#Profiling-caveats" title="Permalink"></a></h3><p>The same function, but with keyword arguments, can be used to change these settings, however these settings are system dependent. For example on Windows, there is a known issue that does not allow to sample faster than at <code>0.003s</code> and even on Linux based system this may not do much. There are some further caveat specific to Julia:</p><ul><li>When running profile from REPL, it is usually dominated by the interactive part which spawns the task and waits for it&#39;s completion.</li><li>Code has to be run before profiling in order to filter out all the type inference and interpretation stuff. (Unless compilation is what we want to profile.)</li><li>When the execution time is short, the sampling may be insufficient -&gt; run multiple times.</li></ul><p>Let&#39;s start with <code>ProfileSVG</code> and save the output to <code>profile.svg</code></p><pre><code class="nohighlight hljs">using ProfileSVG
ProfileSVG.save(&quot;/tmp/profile.svg&quot;)</code></pre><p>which shows the output of the profiler as a flame graph which reads as follows:</p><ul><li>the width of the bar corresponds to time spent in the function</li><li>red colored bars indicate type instabilities</li><li>functions in bottom bars calles functions on top of upper bars</li></ul><p>Function name contains location in files and particular line number called. GTK version is even &quot;clickable&quot; and opens the file in default editor.</p><h2 id="continueing-with-the-example-Numpy-10x..."><a class="docs-heading-anchor" href="#continueing-with-the-example-Numpy-10x...">continueing with the example Numpy 10x...</a><a id="continueing-with-the-example-Numpy-10x...-1"></a><a class="docs-heading-anchor-permalink" href="#continueing-with-the-example-Numpy-10x..." title="Permalink"></a></h2><p>We can see that the function is type stable and 2/3 of the time is spent in lines 3 and 4, which allocates arrays</p><pre><code class="nohighlight hljs">[cos(t0) - 1im*sin(t0)  0; 
 0  cos(t0) + 1im*sin(t0)]</code></pre><p>and</p><pre><code class="nohighlight hljs">[cos(t1) - 1im*sin(t1)  0; 
 0  cos(t1) + 1im*sin(t1)]</code></pre><p>.</p><p>Looking at function <code>f</code>, we see that in every call, it has to allocate arrays <code>m0</code> and <code>m1</code> on the heap. The allocation on heap is expensive, because it requires interaction with the operating system. Can we avoid it? Repeated allocation can be frequently avoided by:</p><ul><li>preallocating arrays</li><li>allocating objects on stack, which does not involve interacion with OS (but can be used in limited cases.)</li></ul><p>Preallocation</p><pre><code class="language-julia hljs">function f!(m0, m1, p, u)   										# line 1 
    t0,t1 = p 														# line 2
    m0[1,1] = cos(t0) - 1im*sin(t0)									# line 3
    m0[2,2] = cos(t0) + 1im*sin(t0)									# line 4
    m1[1,1] = cos(t1) - 1im*sin(t1)									# line 5
    m1[2,2] = cos(t1) + 1im*sin(t1)									# line 6
    r = m1*m0*u 													# line 7
    return abs(r[1])^2 												# line 8
end

function g2(p,n)
	u = [1. ; 0.]
	m0 = [[cos(p[1]) - 1im*sin(p[1])  0]; [0  cos(p[1]) + 1im*sin(p[1])]]	# line 3
    m1 = [[cos(p[2]) - 1im*sin(p[2])  0]; [0  cos(p[2]) + 1im*sin(p[2])]]
    return [f!(m0, m1, p[:,i], u) for i=1:n]
end</code></pre><h2 id="Detour-no-2.-Benchmarking"><a class="docs-heading-anchor" href="#Detour-no-2.-Benchmarking">Detour no 2. Benchmarking</a><a id="Detour-no-2.-Benchmarking-1"></a><a class="docs-heading-anchor-permalink" href="#Detour-no-2.-Benchmarking" title="Permalink"></a></h2><h2 id="Coming-back-from-detour"><a class="docs-heading-anchor" href="#Coming-back-from-detour">Coming back from detour</a><a id="Coming-back-from-detour-1"></a><a class="docs-heading-anchor-permalink" href="#Coming-back-from-detour" title="Permalink"></a></h2><pre><code class="nohighlight hljs">using BenchmarkTools

julia&gt; @benchmark g(p,n)
BenchmarkTools.Trial: 5 samples with 1 evaluation.
 Range (min … max):  1.168 s …   1.199 s  ┊ GC (min … max): 11.57% … 13.27%
 Time  (median):     1.188 s              ┊ GC (median):    11.91%
 Time  (mean ± σ):   1.183 s ± 13.708 ms  ┊ GC (mean ± σ):  12.10% ±  0.85%

  █ █                                 █ █                 █
  █▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ ▁
  1.17 s         Histogram: frequency by time         1.2 s &lt;

 Memory estimate: 1.57 GiB, allocs estimate: 23000002.</code></pre><pre><code class="nohighlight hljs">julia&gt; @benchmark g2(p,n)
BenchmarkTools.Trial: 11 samples with 1 evaluation.
 Range (min … max):  413.167 ms … 764.393 ms  ┊ GC (min … max):  6.50% … 43.76%
 Time  (median):     426.728 ms               ┊ GC (median):     6.95%
 Time  (mean ± σ):   460.688 ms ± 102.776 ms  ┊ GC (mean ± σ):  12.85% ± 11.04%

  ▃█ █
  ██▇█▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇ ▁
  413 ms           Histogram: frequency by time          764 ms &lt;

 Memory estimate: 450.14 MiB, allocs estimate: 4000021.
</code></pre><p>We can see that we have approximately 3-fold improvement.</p><p>Let&#39;s profile again and do not forget to use <code>Profile.clear()</code> to clear already stored probes.</p><pre><code class="nohighlight hljs">Profile.clear()
@profile g2(p,n)
ProfileSVG.save(&quot;/tmp/profile2.svg&quot;)</code></pre><p>What we can see the profiler now? 	- we spend a lot of time in <code>similar</code> in <code>matmul</code>, which is again an allocation of results for storing output of multiplication on line 7 matrix <code>r</code>. 	- the trigonometric operations on line 3-6 are very costly 	- Slicing <code>p</code> always allocates a new array and performs a deep copy.</p><p>Let&#39;s try to get rid of these.</p><pre><code class="language-julia hljs">using LinearAlgebra
@inline function initm!(m, t)
    st, ct = sincos(t) 
    @inbounds m[1,1] = Complex(ct, -st)								
    @inbounds m[2,2] = Complex(ct, st)   								
end

function f1!(r1, r2, m0, m1, t0, t1, u)   					
    initm!(m0, t0)
    initm!(m1, t1)
    mul!(r1, m0, u)
    mul!(r2, m1, r1)
    return @inbounds abs(@inbounds r2[1])^2
end

function g2(p,n)
	u = [1. ; 0.]
	m0 = [[cos(p[1]) - 1im*sin(p[1])  0]; [0  cos(p[1]) + 1im*sin(p[1])]]
    m1 = [[cos(p[2]) - 1im*sin(p[2])  0]; [0  cos(p[2]) + 1im*sin(p[2])]]
    r1 = m0*u
    r2 = m1*r1
    return [f1!(r1, r2, m0, m1, p[1,i], p[2,i], u) for i=1:n]
end</code></pre><pre><code class="language-julia hljs">julia&gt; @benchmark g2(p,n)
 Range (min … max):  193.922 ms … 200.234 ms  ┊ GC (min … max): 0.00% … 1.67%
 Time  (median):     195.335 ms               ┊ GC (median):    0.00%
 Time  (mean ± σ):   196.003 ms ±   1.840 ms  ┊ GC (mean ± σ):  0.26% ± 0.61%

  █▁  ▁ ██▁█▁  ▁█ ▁ ▁ ▁        ▁   ▁     ▁   ▁▁   ▁  ▁        ▁
  ██▁▁█▁█████▁▁██▁█▁█▁█▁▁▁▁▁▁▁▁█▁▁▁█▁▁▁▁▁█▁▁▁██▁▁▁█▁▁█▁▁▁▁▁▁▁▁█ ▁
  194 ms           Histogram: frequency by time          200 ms &lt;

 Memory estimate: 7.63 MiB, allocs estimate: 24.
 ```
Notice that now, we are about six times faster than the first solution, albeit passing the pre code is getting messy. Also notice that we spent a very little time in garbage collector. Running the profiler, we see that there is very little what we can do now. May-be, remove 
bounds checks and make the code a bit nicer.

Let&#39;s look at solution from a Discourse</code></pre><p>julia using StaticArrays, BenchmarkTools</p><p>function f(t0,t1)     cis0, cis1 = cis(t0), cis(t1)     m0 = @SMatrix [ conj(cis0) 0 ; 0 cis0]     m1 = @SMatrix [ conj(cis1) 0 ; 0 cis1]     r = m1 * (m0 * @SVector [1. , 0.])     return abs2(r[1]) end</p><p>g(p) = [f(p[1,i],p[2,i]) for i in axes(p,2)]</p><pre><code class="nohighlight hljs"></code></pre><p>julia&gt; @benchmark g(p)  Range (min … max):  36.076 ms … 43.657 ms  ┊ GC (min … max): 0.00% … 9.96%  Time  (median):     37.948 ms              ┊ GC (median):    0.00%  Time  (mean ± σ):   38.441 ms ±  1.834 ms  ┊ GC (mean ± σ):  1.55% ± 3.60%</p><pre><code class="nohighlight hljs">    █▃▅   ▅▂  ▂</code></pre><p>▅▇▇███████▅███▄████▅▅▅▅▄▇▅▇▄▇▄▁▄▇▄▄▅▁▄▁▄▁▄▅▅▁▁▅▁▁▅▄▅▄▁▁▁▁▁▅ ▄   36.1 ms         Histogram: frequency by time        43.4 ms &lt;</p><p>Memory estimate: 7.63 MiB, allocs estimate: 2.</p><pre><code class="nohighlight hljs">We can see that it is six-times faster than ours while also being much nicer to read and 
having almost no allocations. Where is the catch?
It uses `StaticArrays` which offers linear algebra primitices performant for vectors and matrices of small size. They are allocated on stack, therefore there is no pressure of GarbageCollector and the type is specialized on size of matrices (unlike regular matrices) works on arrays of an sizes. This allows the compiler to perform further optimizations like unrolling loops, etc.

What we have learned so far?
- Profiler is extremely useful in identifying functions, where your code spends most time.
- Memory allocation (on heap to be specific) can be very bad for the performance. We can generally avoided by pre-allocation (if possible) or allocating on the stack (Julia offers increasingly larger number of primitives for hits. We have already seen StaticArrays, DataFrames now offers for example String3, String7, String15, String31).
- Benchmarking is useful for comparison of solutions


## Replacing deep copies with shallow copies (use view if possible)

Let&#39;s look at the following function computing mean of a columns</code></pre><p>julia function cmean(x::AbstractMatrix{T}) where {T} 	o = zeros(T, size(x,1)) 	n = 0  	for i in axes(x, 2) 		o .+= x[:,i] 	end 	n &gt; 0 ? o ./ n : o  end x = randn(2, 10000)</p><pre><code class="nohighlight hljs"></code></pre><p>@benchmark cmean(x) BenchmarkTools.Trial: 10000 samples with 1 evaluation.  Range (min … max):  371.018 μs …   3.291 ms  ┊ GC (min … max): 0.00% … 83.30%  Time  (median):     419.182 μs               ┊ GC (median):    0.00%  Time  (mean ± σ):   482.785 μs ± 331.939 μs  ┊ GC (mean ± σ):  9.91% ± 12.02%</p><p>▃█▄▃▃▂▁                                                       ▁   ████████▇▆▅▃▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▇██ █   371 μs        Histogram: log(frequency) by time       2.65 ms &lt;</p><p>Memory estimate: 937.59 KiB, allocs estimate: 10001.</p><pre><code class="nohighlight hljs">What we see that function is performing more than 10000 allocations. They come from `x[:,i]` which allocates a new memory and copies the content. In this case, this is completely unnecessary, as the content of the array `x` is never modified. We can avoid it by creating a `view` into an `x`, which you can imagine as a pointer to `x` which automatically adjust the bounds. Views can be constructed either using a function call `view(x, axes...)` or using a convenience macro `@view ` which turns the usual notation `x[...]` to `view(x, ...)`
</code></pre><p>julia function view_cmean(x::AbstractMatrix{T}) where {T} 	o = zeros(T, size(x,1)) 	for i in axes(x, 2) 		o .+= @view x[:,i] 	end 	n = size(x,2) 	n &gt; 0 ? o ./ n : o  end</p><pre><code class="nohighlight hljs">
We obtain instantly a 10-fold speedup</code></pre><p>julia julia&gt; @benchmark view_cmean(x) BenchmarkTools.Trial: 10000 samples with 1 evaluation.  Range (min … max):  36.802 μs … 166.260 μs  ┊ GC (min … max): 0.00% … 0.00%  Time  (median):     41.676 μs               ┊ GC (median):    0.00%  Time  (mean ± σ):   42.936 μs ±   9.921 μs  ┊ GC (mean ± σ):  0.00% ± 0.00%</p><p>▂ █▆█▆▂      ▁▁ ▁ ▁                                          ▂   █▄█████████▇▅██▆█▆██▆▆▇▆▆▆▆▇▆▅▆▆▅▅▁▅▅▆▇▆▆▆▆▄▃▆▆▆▄▆▄▅▅▄▆▅▆▅▄▆ █   36.8 μs       Histogram: log(frequency) by time      97.8 μs &lt;</p><p>Memory estimate: 96 bytes, allocs estimate: 1.</p><pre><code class="nohighlight hljs">
## Traversing arrays in the right order matter

Let&#39;s now compute rowmean using the function similar to `cmean` and since we have learnt from the above, we use the `view` to have non-allocating version</code></pre><p>julia function rmean(x::AbstractMatrix{T}) where {T} 	o = zeros(T, size(x,2)) 	for i in axes(x, 1) 		o .+= @view x[i,:] 	end 	n = size(x,1) 	n &gt; 0 ? o ./ n : o  end</p><pre><code class="nohighlight hljs"></code></pre><p>julia x = randn(10000, 2) @benchmark rmean(x) BenchmarkTools.Trial: 10000 samples with 1 evaluation.  Range (min … max):  44.165 μs … 194.395 μs  ┊ GC (min … max): 0.00% … 0.00%  Time  (median):     46.654 μs               ┊ GC (median):    0.00%  Time  (mean ± σ):   48.544 μs ±  10.940 μs  ┊ GC (mean ± σ):  0.00% ± 0.00%</p><p>█▆█▇▄▁            ▁                                          ▂   ██████▇▇▇▇▆▇▅██▇█▇█▇▆▅▄▄▅▅▄▄▄▄▂▄▅▆▅▅▅▆▅▅▅▆▄▆▄▄▅▅▄▅▄▄▅▅▅▅▄▄▃▅ █   44.2 μs       Histogram: log(frequency) by time       108 μs &lt;</p><p>Memory estimate: 192 bytes, allocs estimate: 2.</p><pre><code class="nohighlight hljs">The above seems OK and the speed is comparable to our tuned `cmean`.  But, can we actually do better? We have to realize that when we are accessing slices in the matrix `x`, they are not aligned in the memory. Recall that Julia is column major (like Fortran and unlike C and Python), which means that consecutive arrays of memory are along columns. i.e for a matrix with n rows and m columns they are aligned as </code></pre><p>1 | n + 1 | 2n + 1 | ⋯ | (m-1)n + 1 2 | n + 2 | 2n + 2 | ⋯ | (m-1)n + 2 3 | n + 3 | 2n + 3 | ⋯ | (m-1)n + 3 ⋮ |   ⋮   |    ⋮   | ⋯ |        ⋮   n |  2n   |    3n  | ⋯ |       mn</p><pre><code class="nohighlight hljs">accessing non-consecutively is really bad for cache, as we have to load the memory into a cache line and use a single entry (in case of Float64 it is 8 bytes) out of it, discard it and load another one. If cache line has length 32 bytes, then we are wasting remaining 24 bytes. Therefore, we rewrite `rmean` to access the memory in consecutive blocks as follows, where we essentially sum the matrix column by columns.</code></pre><p>julia function aligned_rmean(x::AbstractMatrix{T}) where {T} 	o = zeros(T, size(x,2)) 	for i in axes(x, 2) 		o[i] = sum(@view x[:, i]) 	end 	n = size(x, 1) 	n &gt; 0 ? o ./ n : o  end</p><p>aligned_rmean(x) ≈ rmean(x)</p><pre><code class="nohighlight hljs">
</code></pre><p>julia julia&gt; @benchmark aligned_rmean(x) BenchmarkTools.Trial: 10000 samples with 10 evaluations.  Range (min … max):  1.988 μs …  11.797 μs  ┊ GC (min … max): 0.00% … 0.00%  Time  (median):     2.041 μs               ┊ GC (median):    0.00%  Time  (mean ± σ):   2.167 μs ± 568.616 ns  ┊ GC (mean ± σ):  0.00% ± 0.00%</p><p>█▇▄▂▂▁▁ ▁  ▂▁                                               ▂   ██████████████▅▅▃▁▁▁▁▁▄▅▄▁▅▆▆▆▇▇▆▆▆▆▅▃▅▅▄▅▅▄▄▄▃▃▁▁▁▄▁▁▄▃▄▃▆ █   1.99 μs      Histogram: log(frequency) by time      5.57 μs &lt;</p><p>Memory estimate: 192 bytes, allocs estimate: 2.</p><pre><code class="nohighlight hljs">Running the benchmark shows that we have about 20x speedup and we are on par with Julia&#39;s built-in functions.

**Remark tempting it might be, there is actually nothing we can do to speed-up the `cmean` function. This trouble is inherent to the processor desing and you should be careful how you align things in the memory, such that it is performant in your project**

Detecting this type of inefficiencies is generally difficult, and requires processor assisted measurement. `LIKWID.jl` is a wrapper for a LIKWID library providing various processor level statistics, like throughput, cache misses

## Type stability
Sometimes it happens that we create a non-stable code, which might be difficult to spot at first, for a non-trained eye. A prototypical example of such bug is as follows</code></pre><p>julia function poor_sum(x) 	s = 0 	n = length(x) 	for i in 1:n 		s += x[i] 	end 	s end</p><pre><code class="nohighlight hljs"></code></pre><p>julia x = randn(10^8); julia&gt; @benchmark poor_sum(x) BenchmarkTools.Trial: 23 samples with 1 evaluation.  Range (min … max):  222.055 ms … 233.552 ms  ┊ GC (min … max): 0.00% … 0.00%  Time  (median):     225.259 ms               ┊ GC (median):    0.00%  Time  (mean ± σ):   225.906 ms ±   3.016 ms  ┊ GC (mean ± σ):  0.00% ± 0.00%</p><p>▁ ▁ ▁▁█  ▁▁  ▁ ▁█ ▁ ▁ ▁ ▁ ▁    ▁▁▁▁                      ▁  ▁   █▁█▁███▁▁██▁▁█▁██▁█▁█▁█▁█▁█▁▁▁▁████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁█ ▁   222 ms           Histogram: frequency by time          234 ms &lt;</p><p>Memory estimate: 16 bytes, allocs estimate: 1.</p><pre><code class="nohighlight hljs"> 
Can we do better? Let&#39;s look what profiler says.</code></pre><p>using Profile, ProfileSVG Profile.clear() @profile  poor_sum(x) ProfileSVG.save(&quot;/tmp/profile4.svg&quot;)</p><pre><code class="nohighlight hljs">THe profiler does not show any red, which means that according to the profilerthe code is type stable (and so does the `@code_typed poor_sum(x)` does not show anything bad.) Yet, we can see that the fourth line of the `poor_sum` function takes unusually long (there is a white area above, which means that the time spend in childs of that line (iteration and sum) does the sum to the time spent in the line, which is fishy). 

A close lookup on the code reveals that `s` is initialized as `Int64`, because `typeof(0)` is `Int64`. But then in the loop, we add to `s` a `Float64` because `x` is `Vector{Float64}`, which means during the execution, the type `s` changes the type.

So why nor compiler nor `@code_typed(poor_sum(x))` warns us about the type instability? This is because of the optimization called **small unions**, where Julia can optimize &quot;small&quot; type instabilitites (recall the second lecture).

We can fix it for example by initializing `x` to be the zero of an element type of the array `x` (though this solution technically assumes `x` is an array, which means that `poor_sum` will not work for generators)</code></pre><p>julia function stable_sum(x) 	s = zero(eltype(x)) 	n = length(x) 	for i in 1:n 		s += x[i] 	end 	s end</p><pre><code class="nohighlight hljs">
But there is no difference, due to small union optimization (the above would kill any performance in older versions.)</code></pre><p>julia julia&gt; @benchmark stable_sum(x) BenchmarkTools.Trial: 42 samples with 1 evaluation.  Range (min … max):  119.491 ms … 123.062 ms  ┊ GC (min … max): 0.00% … 0.00%  Time  (median):     120.535 ms               ┊ GC (median):    0.00%  Time  (mean ± σ):   120.687 ms ± 819.740 μs  ┊ GC (mean ± σ):  0.00% ± 0.00%</p><pre><code class="nohighlight hljs">        █</code></pre><p>▅▁▅▁▅▅██▅▁█▁█▁██▅▁█▅▅▁█▅▁█▁█▅▅▅█▁▁▁▁▁▁▁▅▁▁▁▁▁▅▁▅▁▁▁▁▁▁▅▁▁▁▁▁▅ ▁   119 ms           Histogram: frequency by time          123 ms &lt;</p><p>Memory estimate: 16 bytes, allocs estimate: 1.</p><pre><code class="nohighlight hljs">

The main problem with the above formulation is that Julia is checking that getting element of arrays from `x[i]` is within bounds. We can remove the check using `@inbounds` macro.</code></pre><p>julia function inbounds_sum(x) 	n = length(x) 	n == 0 &amp;&amp; return(zero(eltype(x))) 	n == 1 &amp;&amp; return(x[1])     @inbounds a1 = x[1]     @inbounds a2 = x[2]     v = a1 + a2     for i = 3 : n         @inbounds ai = x[i]         v += ai     end     v end</p><pre><code class="nohighlight hljs">
This gives us few orders of magnitude speed improvements. </code></pre><p>julia&gt; @benchmark inbounds_sum(x) BenchmarkTools.Trial: 40 samples with 1 evaluation.  Range (min … max):  123.642 ms … 130.257 ms  ┊ GC (min … max): 0.00% … 0.00%  Time  (median):     125.142 ms               ┊ GC (median):    0.00%  Time  (mean ± σ):   125.691 ms ±   1.783 ms  ┊ GC (mean ± σ):  0.00% ± 0.00%</p><pre><code class="nohighlight hljs">██▃  ▃ ▃ █ █       ▃ ▃        ▃</code></pre><p>▇▁███▇▁█▁█▁█▇█▇▇▇▇▁▇▁█▁█▁▁▇▁▁▁▁▁█▁▇▁▁▇▁▁▁▇▁▁▁▁▇▁▁▁▁▁▁▁▁▁▁▇▇▁▇ ▁   124 ms           Histogram: frequency by time          130 ms &lt;</p><p>Memory estimate: 16 bytes, allocs estimate: 1.</p><pre><code class="nohighlight hljs"></code></pre><p>julia function simd_sum(x) 	n = length(x) 	n == 0 &amp;&amp; return(zero(eltype(x))) 	n == 1 &amp;&amp; return(x[1])     @inbounds a1 = x[1]     @inbounds a2 = x[2]     v = a1 + a2     @simd for i = 3 : n         @inbounds ai = x[i]         v += ai     end     v end</p><pre><code class="nohighlight hljs"></code></pre><p>julia&gt; @benchmark simd_sum(x) BenchmarkTools.Trial: 90 samples with 1 evaluation.  Range (min … max):  50.854 ms … 62.260 ms  ┊ GC (min … max): 0.00% … 0.00%  Time  (median):     54.656 ms              ┊ GC (median):    0.00%  Time  (mean ± σ):   55.630 ms ±  3.437 ms  ┊ GC (mean ± σ):  0.00% ± 0.00%</p><pre><code class="nohighlight hljs">█  ▂     ▄ ▂                    ▂    ▂            ▄</code></pre><p>▄▆█▆▁█▄██▁▁█▆██▆▄█▁▆▄▁▆▆▄▁▁▆▁▁▁▁▄██▁█▁▁█▄▄▆▆▄▄▁▄▁▁▁▄█▁▆▁▆▁▆ ▁   50.9 ms         Histogram: frequency by time        62.1 ms &lt;</p><p>Memory estimate: 16 bytes, allocs estimate: 1.</p><pre><code class="nohighlight hljs">

## Global variables introduces type instability (Avoid non-const globals)</code></pre><p>julia function implicit_sum() 	n = length(x) 	n == 0 &amp;&amp; return(zero(eltype(x))) 	n == 1 &amp;&amp; return(x[1])     @inbounds a1 = x[1]     @inbounds a2 = x[2]     v = a1 + a2     @simd for i = 3 : n         @inbounds ai = x[i]         v += ai     end     v end</p><pre><code class="nohighlight hljs">
</code></pre><p>julia&gt; @benchmark implicit_sum() BenchmarkTools.Trial: 1 sample with 1 evaluation.  Single result which took 10.837 s (11.34% GC) to evaluate,  with a memory estimate of 8.94 GiB, over 499998980 allocations.</p><pre><code class="nohighlight hljs">What? The same function where I made the parameters to be implicit has just turned **nine orders of magnitude** slower? 

Let&#39;s look what the debugger says</code></pre><p>julia Profile.clear() x = randn(10^4) @profile implicit<em>sum() ProfileSVG.save(&quot;/tmp/profile</em>5.svg&quot;)</p><pre><code class="nohighlight hljs">which does not say anything except that there is a huge type-instability (red bar). In fact, the whole computation is dominated by Julia constantly determining the type (of something).

We have several tools that can help us to determine the type instability. 
- `@code_typed implicit_sum()`
- Cthulhu as `@descend implicit_sum()`
- JET (available with the nightly build of Julia and 1.7 pre-releases) `@report_opt implicit_sum()`

All of these tools tells us that the Julia&#39;s compiler cannot determine the type of `x`. But why? I can just invoke `typeof(x)` and I know immediately the type of `x`. 

To understand the problem, you have to think about the compiler.
1. You define function `implicit_sum().`
2. If you call `implicit_sum` and `x` has not exist, Julia will happily crash.
3. If you call `implicit_sum` and `x` exist, the function will gives you the result (albeit slowly). At this moment, Julia has to specialize `implicit_sum`. It has two options how to behave with respect to `x`. 
	a. She can assume that type of `x` is the current `typeof(x)` but that would mean that if a user redefines x and change the type, the specialization of the function `implicit_sum` will assume the wrong type of `x` and it can have unexpected results.
	b. She can take safe approach and determine the type of `x` inside the function `implicit_sum` and behave accordingly (recall that julia is dynamically type). Yet, not knowing the type precisely is absolute disaster for performance.

Notice the compiler dispatches on the name of the function and type of its arguments, hence, the compiler cannot create different versions of `implicit_sum` for different types of `x`, since it is not in argument, hence the dynamic resolution of types `x` inside `implicit_sum` function.

Julia takes the **safe approach**, which we can verify that although the `implicit_sum` was specialized (compiled) when `x` was `Vector{Float64}`, it works for other types</code></pre><p>julia x = rand(Int, 1000) implicit<em>sum() ≈ sum(x) x = map(x -&gt; Complex(x...), zip(rand(1000), rand(1000))) implicit</em>sum() ≈ sum(x)</p><pre><code class="nohighlight hljs">
This means, using global variables inside functions without passing them as arguments ultimetly leads to type-instability. What are the solutions

### Declaring `x` as const
We can declare `x` as const, which tells the compiler that `x` will not change (and for the compiler mainly indicates **that type of `x` will not change**).

Let&#39;s see that, but restart the julia before trying</code></pre><p>julia using BenchmarkTools function implicit_sum() 	n = length(x) 	n == 0 &amp;&amp; return(zero(eltype(x))) 	n == 1 &amp;&amp; return(x[1])     @inbounds a1 = x[1]     @inbounds a2 = x[2]     v = a1 + a2     @simd for i = 3 : n         @inbounds ai = x[i]         v += ai     end     v end const x = randn(10^8);</p><pre><code class="nohighlight hljs">
after benchmarking we see that the speed is the same as of `simd_sum()`.</code></pre><p>julia julia&gt; @benchmark implicit_sum() BenchmarkTools.Trial: 99 samples with 1 evaluation.  Range (min … max):  47.864 ms … 58.365 ms  ┊ GC (min … max): 0.00% … 0.00%  Time  (median):     50.042 ms              ┊ GC (median):    0.00%  Time  (mean ± σ):   50.479 ms ±  1.598 ms  ┊ GC (mean ± σ):  0.00% ± 0.00%</p><pre><code class="nohighlight hljs">      ▂ █▂▂▇ ▅  ▃</code></pre><p>▃▁▃▁▁▁▁▇██████▅█▆██▇▅▆▁▁▃▅▃▃▁▃▃▁▃▃▁▁▃▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▃ ▁   47.9 ms         Histogram: frequency by time        57.1 ms &lt;</p><p>Memory estimate: 0 bytes, allocs estimate: 0.</p><pre><code class="nohighlight hljs">
### Barier function
The reason, why the `implicit_sum` is so slow that everytime the function invokes `getindex` and `+`, it has to resolve types. The solution would be to limit the number of resolutions, which can done by passing all parameters to inner function as follows.
</code></pre><p>julia using BenchmarkTools function simd_sum(x) 	n = length(x) 	n == 0 &amp;&amp; return(zero(eltype(x))) 	n == 1 &amp;&amp; return(x[1])     @inbounds a1 = x[1]     @inbounds a2 = x[2]     v = a1 + a2     @simd for i = 3 : n         @inbounds ai = x[i]         v += ai     end     v end</p><p>function barrier<em>sum() 	simd</em>sum(x) end x = randn(10^8);</p><pre><code class="nohighlight hljs"></code></pre><p>julia @benchmark barrier_sum() BenchmarkTools.Trial: 93 samples with 1 evaluation.  Range (min … max):  50.229 ms … 58.484 ms  ┊ GC (min … max): 0.00% … 0.00%  Time  (median):     53.882 ms              ┊ GC (median):    0.00%  Time  (mean ± σ):   54.064 ms ±  2.892 ms  ┊ GC (mean ± σ):  0.00% ± 0.00%</p><p>▂▆█                                          ▆▄   ▆█████▆▄█▆▄▆▁▄▄▄▄▁▁▄▁▄▄▆▁▄▄▄▁▁▄▁▁▄▁▁▆▆▁▁▄▄▁▄▆████▄▆▄█▆▄▄▄▄█ ▁   50.2 ms         Histogram: frequency by time        58.4 ms &lt;</p><p>Memory estimate: 16 bytes, allocs estimate: 1.</p><pre><code class="nohighlight hljs">

## Detour 3. JET
Abstract Interpreter JET

 of the childs does not match. Why is that? What we can see is Julia&#39;s optimization called **Small Unions** (we have talked about this in lecture on Type stability).


## return from Detour</code></pre><p>julia using JET @report<em>opt implicit</em>sum() @report<em>opt barrier</em>sum()</p><pre><code class="nohighlight hljs">
## Boxing in closure
Recall closure is a function which contains some parameters contained 

An example of closure from Jet.jl</code></pre><p>julia function abmult(r::Int)    if r &lt; 0        r = -r    end</p><h1 id="the-closure-assigned-to-f-make-the-variable-r-captured"><a class="docs-heading-anchor" href="#the-closure-assigned-to-f-make-the-variable-r-captured">the closure assigned to <code>f</code> make the variable <code>r</code> captured</a><a id="the-closure-assigned-to-f-make-the-variable-r-captured-1"></a><a class="docs-heading-anchor-permalink" href="#the-closure-assigned-to-f-make-the-variable-r-captured" title="Permalink"></a></h1><p>f = x -&gt; x * r    return f end;</p><pre><code class="nohighlight hljs">
Another example of closure counting the error and printing it every `steps`</code></pre><p>julia function initcallback(; steps =10) 	i = 0 	ts = time() 	y = 0.0 	cby = function evalcb(_y) 		i += 1 		y += _y 		if mod(i, steps) == 0	% line 4 			l = y / steps 			y = 0.0 			println(i, &quot;: loss: &quot;, l,&quot; time per step: &quot;,round((time() - ts)/steps, sigdigits = 2)) 			ts = time() 		end 	end 	cby end</p><p>cby = initcallback() for i in 1:100 	cby(rand()) end</p><pre><code class="nohighlight hljs">
Sometimes, Julia&#39;s compiler cannot reason about types it has closed over. The problem is similar to the problem we have shown above in `poor_sum` and in `implicit_sum`.</code></pre><p>function simulation() 	cby = initcallback(;steps = 10000)	#intentionally disable printing 	for i in 1:1000 		cby(sin(rand())) 	end end</p><p>@benchmark simulation()</p><pre><code class="nohighlight hljs"></code></pre><p>using Profile, ProfileSVG Profile.clear() @profile (for i in 1:100; simulation(); end) ProfileSVG.save(&quot;/tmp/profile.svg&quot;)</p><pre><code class="nohighlight hljs">We see a red bars in lines 4 and 8 of evalcb, whcih is 
- performance tweaks
	+ boxing in closures
	+ differnce between named tuple and dict
	+ IO</code></pre><ul><li>create a list of examples for the lecture</li></ul><h2 id="Using-named-tuple-instead-of-dict"><a class="docs-heading-anchor" href="#Using-named-tuple-instead-of-dict">Using named tuple instead of dict</a><a id="Using-named-tuple-instead-of-dict-1"></a><a class="docs-heading-anchor-permalink" href="#Using-named-tuple-instead-of-dict" title="Permalink"></a></h2><pre><code class="language-julia hljs">params_dict = Dict(:stepsize =&gt; 0.01, :h =&gt; 0.001, :iters =&gt; 500)
params_tuple = (;stepsize = 0.01, h=0.001, iters=500)

function find_min!(f, x, p)
	for i in 1:p[:iters]
		x̃ = x + p[:h]
		fx = f(x)
		x -= p[:stepsize] * (f(x̃) - fx)/p[:h]
	end
	x
end

x₀ = rand()
f = x -&gt; x^2
find_min!(f, x₀, params_tuple)
@btime find_min!($f, $x₀, $params_dict)
@btime find_min!($f, $x₀, $params_tuple)</code></pre><h2 id="Performance-of-captured-variable"><a class="docs-heading-anchor" href="#Performance-of-captured-variable">Performance of captured variable</a><a id="Performance-of-captured-variable-1"></a><a class="docs-heading-anchor-permalink" href="#Performance-of-captured-variable" title="Permalink"></a></h2><ul><li>inspired by https://docs.julialang.org/en/v1/manual/performance-tips/#man-performance-captured</li><li>an example of closure seen in previous lectures, reference <code>x</code> has to be included </li></ul><pre><code class="language-julia hljs">x = rand(1000)

function adder(shift)
    return y -&gt; shift + y
end

function adder_typed(shift::Float64)
    return y -&gt; shift + y
end

function adder_let(shift::Float64)
	f = let shift=shift
		y -&gt; shift + y
    end
    return f
end

f = adder(3.0)
ft = adder_typed(3.0)
fl = adder_let(3.0)

@btime f.($x);
@btime ft.($x);
@btime fl.($x);
@btime $x .+ 3.0;
</code></pre><ul><li>cannot get the same performance as native call, might be affected by broadcasting (?)</li><li><code>fl</code> should attain the same performance as native call</li></ul><h2 id="Don&#39;t-use-IO-unless-you-have-to"><a class="docs-heading-anchor" href="#Don&#39;t-use-IO-unless-you-have-to">Don&#39;t use IO unless you have to</a><a id="Don&#39;t-use-IO-unless-you-have-to-1"></a><a class="docs-heading-anchor-permalink" href="#Don&#39;t-use-IO-unless-you-have-to" title="Permalink"></a></h2><ul><li>debug printing in performance critical code should be kept to minimum or using in memory/file based logger in stdlib <code>Logging.jl</code></li></ul><pre><code class="language-julia hljs">function find_min!(f, x, p; verbose=true)
	for i in 1:p[:iters]
		x̃ = x + p[:h]
		fx = f(x)
		x -= p[:stepsize] * (f(x̃) - fx)/p[:h]
		verbose &amp;&amp; println(&quot;x = &quot;, x, &quot; | f(x) = &quot;, fx)
	end
	x
end

@btime find_min!($f, $x₀, $params_tuple; verbose=true)
@btime find_min!($f, $x₀, $params_tuple; verbose=false)</code></pre><ul><li>interpolation of strings is even worse https://docs.julialang.org/en/v1/manual/performance-tips/#Avoid-string-interpolation-for-I/O</li></ul><pre><code class="language-julia hljs">function find_min!(f, x, p; verbose=true)
	for i in 1:p[:iters]
		x̃ = x + p[:h]
		fx = f(x)
		x -= p[:stepsize] * (f(x̃) - fx)/p[:h]
		verbose &amp;&amp; println(&quot;x = $x | f(x) = $fx&quot;)
	end
	x
end
@btime find_min!($f, $x₀, $params_tuple; verbose=true)</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../lecture_04/hw/">« Homework</a><a class="docs-footer-nextpage" href="../lab/">Lab »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.10 on <span class="colophon-date" title="Thursday 28 October 2021 11:43">Thursday 28 October 2021</span>. Using Julia version 1.6.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
