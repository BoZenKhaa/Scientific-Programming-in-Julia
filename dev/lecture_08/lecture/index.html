<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Lecture · Scientific Programming in Julia</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://JuliaTeachingCTU.github.io/Scientific-Programming-in-Julia/lecture_08/lecture/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Scientific Programming in Julia logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Scientific Programming in Julia logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Scientific Programming in Julia</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../installation/">Installation</a></li><li><a class="tocitem" href="../../projects/">Projects</a></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Introduction</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/motivation/">Motivation</a></li><li><a class="tocitem" href="../../lecture_01/basics/">Basics</a></li><li><a class="tocitem" href="../../lecture_01/demo/">Examples</a></li><li><a class="tocitem" href="../../lecture_01/outline/">Outline</a></li><li><a class="tocitem" href="../../lecture_01/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_01/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: The power of Type System &amp; multiple dispatch</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_02/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_02/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Design patterns</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_03/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_03/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Packages development, Unit Tests &amp; CI</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_04/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_04/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-8" type="checkbox"/><label class="tocitem" for="menuitem-8"><span class="docs-label">5: Benchmarking, profiling, and performance gotchas</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_05/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_05/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_05/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">6: Language introspection</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_06/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_06/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_06/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">7: Macros</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/lecture/">Lecture</a></li><li><a class="tocitem" href="../../lecture_07/lab/">Lab</a></li><li><a class="tocitem" href="../../lecture_07/hw/">Homework</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox" checked/><label class="tocitem" for="menuitem-11"><span class="docs-label">8: Introduction to automatic differentiation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href>Lecture</a><ul class="internal"><li><a class="tocitem" href="#Motivations-to-study-an-automatic-differentiation"><span>Motivations to study an automatic differentiation</span></a></li><li><a class="tocitem" href="#Theory"><span>Theory</span></a></li><li><a class="tocitem" href="#Let&#39;s-work-an-example"><span>Let&#39;s work an example</span></a></li><li><a class="tocitem" href="#What-are-the-tricks-that-we-can-use?"><span>What are the tricks that we can use?</span></a></li><li><a class="tocitem" href="#Why-custom-rules"><span>Why custom rules</span></a></li></ul></li></ul></li><li><span class="tocitem">9: Manipulating intermediate representation</span></li><li><span class="tocitem">10: Different levels of parallel programming</span></li><li><span class="tocitem">11: Julia for GPU programming</span></li><li><span class="tocitem">12: Uncertainty propagation in ODE</span></li><li><span class="tocitem">13: Learning ODE from data</span></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">8: Introduction to automatic differentiation</a></li><li class="is-active"><a href>Lecture</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Lecture</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaTeachingCTU/Scientific-Programming-in-Julia/blob/master/docs/src/lecture_08/lecture.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><p>&lt;!– I have once seen a nice tutorial / lecture on AD by Matthew Johnson, but I cannot find it anymore.  –&gt;</p><h1 id="Automatic-Differentiation"><a class="docs-heading-anchor" href="#Automatic-Differentiation">Automatic Differentiation</a><a id="Automatic-Differentiation-1"></a><a class="docs-heading-anchor-permalink" href="#Automatic-Differentiation" title="Permalink"></a></h1><h2 id="Motivations-to-study-an-automatic-differentiation"><a class="docs-heading-anchor" href="#Motivations-to-study-an-automatic-differentiation">Motivations to study an automatic differentiation</a><a id="Motivations-to-study-an-automatic-differentiation-1"></a><a class="docs-heading-anchor-permalink" href="#Motivations-to-study-an-automatic-differentiation" title="Permalink"></a></h2><ul><li>It it support a lot of modern machine learning by allowing quick differentiation of complex mathematical functions. Recall that 1st order optimization methods are ubiquituous in finding parameters of functions.</li><li>AD is interesting to study from the implementation perspective. There are different takes on it with different trade-offs and Julia offers many implementations (some of them of course not maintained anymore).</li><li>We (authors of this course) believe that it is good to understand (at least roughly), how the methods work in order to use them effectively in our research.</li><li>Julia is unique in the effort separating definitions of AD rules from engines that use those rules to perform the AD. This allows authors of libraries to add new rules that would be compatible </li></ul><h2 id="Theory"><a class="docs-heading-anchor" href="#Theory">Theory</a><a id="Theory-1"></a><a class="docs-heading-anchor-permalink" href="#Theory" title="Permalink"></a></h2><p>The differentiation is routine process, as most of the time we can break down complex functions into small pieces that we know, how to differentiate and from that to assemble the gradient of the complex function back. Thus, the essential piece is the differentiation of the composed function, which is called chainrule.</p><p>Specifically, for a composed function <span>$f: \mathbb{R}^n \rightarrow \mathbb{R}^m$</span></p><p><span>$f(x) = f_1(f_2(f_3(\ldots f_n(x))))$</span> </p><p>the gradient with respect to <code>x</code> is equal to</p><p><span>$\frac{\partial f}{\partial x} = \frac{f_1}{\partial y_1} \times \frac{f_2}{\partial y_2} \times \ldots \times \frac{f_n}{\partial y_n}$</span>, </p><p>where</p><p><span>$y_i = f_{i+1}(f_{i+1}(\ldots f_n(x)))$</span> and <span>$y_n = x$</span>.</p><p>How <span>$\frac{f_i}{\partial y_i}$</span> looks like? If <span>$f_i: \mathbb{R} \rightarrow \mathbb{R}$</span>, then <span>$\frac{f_i}{\partial y_i} \in \mathbb{R}$</span> is a real number <span>$\mathbb{R}$</span>. On the other hand if <span>$f_i: \mathbb{R}^{m_i} \rightarrow \mathbb{R}^{n_i}$</span>, then <span>$\frac{f_i}{\partial y_i} \in \mathbb{R}^{n_i,m_i}$</span> is a matrix and the computation of gradient <span>$\frac{\partial f}{\partial x}$</span> boils down to matrix multiplication. Denoting <code>i</code>-th Jacobian as <span>$J_i = \frac{f_i}{\partial y_i}$</span> it holds that <span>$\frac{\partial f}{\partial x} = J_1 \times J_2 \times \ldots \times J_n$</span>. </p><p>Matrix multiplication is generally expensive, as it has at least <span>$O(n^{2.3728596})$</span> (where the devil is hidden in the <span>$O(n)$</span>). The order in which the Jacobians are multiplied has therefore a profound effect on the complexity of the AD engine. While determining the optimal order of multiplication of sequence of matrices is costly (it can be solved using dynamic programming), it is interesting to investigate two cases.</p><p>In the first case, we multiply Jacobians from right to left, i.e. as  <span>$J_1 \times (J_2 \times ( \ldots \times (J_{n-1}) \times J_n))))$</span> or the other way around from left to right <span>$((((J_1 \times J_2) \times J_3) \times \ldots ) \times J_n$</span> The first approach has the advantage that if the input dimension of <span>$f$</span> is smaller than the output dimension (<span>$n &lt; m$</span> where be reminded that <span>$f: \mathbb{R}^n \rightarrow \mathbb{R}^m$</span>), while the latter has the advantage in the oposite case where output dimension is smaller that the input one. In deep learning, one mostly do the opposite case. Also notice that for <code>f</code> of certain structures, it pays-off to do a mixed-mode AD, where some partse are done using forward diff and some parts using reverse diff. </p><h3 id="Forward-mode"><a class="docs-heading-anchor" href="#Forward-mode">Forward mode</a><a id="Forward-mode-1"></a><a class="docs-heading-anchor-permalink" href="#Forward-mode" title="Permalink"></a></h3><p>Initialize the jacobian of <span>$y_n$</span> with respect to <span>$x$</span> to an identity matrix, because as we have stated above <span>$y_n = x$</span>, i.e. <span>$\frac{\partial y_n}{\partial x} = \mathbb{I}$</span>. Iterate <code>i</code> from <code>n</code> down to <code>1</code> as</p><ul><li>calculate the next intermediate ourput as <span>$y_{i-1} = f_i{y_i}$</span> </li><li>calculate Jacobian <span>$J_i = \frac{f_i}{\partial y_i}$</span> at point <span>$y_i$</span></li><li><em>push forward</em> the gradient as <span>$\frac{\partial y_{i-1}}{\partial x} = J_i \times \frac{\partial y_n}{\partial x}$</span></li></ul><p>Notice that </p><ul><li>on the very end, we are left with <code>y = y_0</code> and with <span>$\frac{\partial y_0}{\partial x}$</span>, which is the gradient we wanted to calculate;</li><li>if <code>y</code> is a scalar, then <span>$\frac{\partial y_0}{\partial x}$</span> is a matrix with single row (but in usual notation, it should be single column, right?)</li><li>the jacobian and the output of the function is calculated in one sweep.</li></ul><p>The above is an idealized computation. The real implementation is a bit different, as we will see later.</p><h3 id="Reverse-mode"><a class="docs-heading-anchor" href="#Reverse-mode">Reverse mode</a><a id="Reverse-mode-1"></a><a class="docs-heading-anchor-permalink" href="#Reverse-mode" title="Permalink"></a></h3><p>In reverse mode, the computation of the gradient follow the oposite order. That means. We initialize the Jacobian by <span>$\frac{\partial y}{\partial y_0},$</span> which is again an identity matrix, and perform the computation of jacobians and multiplications in the opposite order. The problem is that to calculate jacobian <span>$J_i$</span>, we need to know <span>$y_i$</span>. Therefore the reverse mode diff requires two passes over the computation graph.</p><ol><li><p>Forward pass: iterate <code>i</code> from <code>n</code> down to <code>1</code> as</p><ul><li>calculate the next intermediate output as <span>$y_{i-1} = f_i(y_i)$</span> </li></ul></li><li><p>Backward pass: iterate <code>i</code> from <code>1</code> down to <code>n</code> as</p><ul><li>calculate Jacobian <span>$J_i = \frac{f_i}{\partial y_i}$</span> at point <span>$y_i$</span></li><li><em>pull back</em> the gradient as <span>$\frac{\partial y_0}{\partial y_{i}} = \frac{\partial y_0}{\partial y_{i-1}} \times J_i$</span></li></ul><p>Notice that we actually cannot compute the backward pass, as we do not know <span>$y_i$</span> at the moment of computing the <code>J_i</code>. Therefore the backward pass needs to be preceeded by forward pass, where we calculate <span>$y_i$</span> as follows</p></li></ol><p>The fact that we need to store intermediate outs has a huge impact on the memory requirements. Therefore when we have been talking few lectures ago that we should avoid excessive memory allocations, here we have an algorithm where the excessive allocation is by design. </p><h2 id="Let&#39;s-work-an-example"><a class="docs-heading-anchor" href="#Let&#39;s-work-an-example">Let&#39;s work an example</a><a id="Let&#39;s-work-an-example-1"></a><a class="docs-heading-anchor-permalink" href="#Let&#39;s-work-an-example" title="Permalink"></a></h2><h3 id="n-to-1-function"><a class="docs-heading-anchor" href="#n-to-1-function"><code>n</code> to <code>1</code> function</a><a id="n-to-1-function-1"></a><a class="docs-heading-anchor-permalink" href="#n-to-1-function" title="Permalink"></a></h3><h3 id="1-to-n-function"><a class="docs-heading-anchor" href="#1-to-n-function"><code>1</code> to <code>n</code> function</a><a id="1-to-n-function-1"></a><a class="docs-heading-anchor-permalink" href="#1-to-n-function" title="Permalink"></a></h3><h2 id="What-are-the-tricks-that-we-can-use?"><a class="docs-heading-anchor" href="#What-are-the-tricks-that-we-can-use?">What are the tricks that we can use?</a><a id="What-are-the-tricks-that-we-can-use?-1"></a><a class="docs-heading-anchor-permalink" href="#What-are-the-tricks-that-we-can-use?" title="Permalink"></a></h2><ul><li>we define custom rules over large functional blocks. For example while we can auto-grad (in theory) matrix product, it is much more efficient to define make a matrix multiplication as one large function, for which we define jacobians (not to say that by doing so, we can dispatch on Blas)</li><li><strong>Invertible functions</strong> When we are differentiating invertible functions, we can calculate intermediate outputs from the </li><li><strong>Checkpointing</strong> We can store the intermediate ouputs only sometime and performed a small forward while doint backward</li><li>** Implicit functions** Differentiating through solvers is possible, since they are mostly iterative. But, using the mathematical equality is easier.</li></ul><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>Reverse mode AD was first published in 1976 by Seppo Linnainmaa, a finnish computer scientist. It was popularized in the end of 80s when applied to training multi-layer perceptrons, which gave rise to the famous <strong>backpropagation</strong> algorithm, which is a special case of reverse mode AD.</p></div></div><h3 id="Implementation"><a class="docs-heading-anchor" href="#Implementation">Implementation</a><a id="Implementation-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation" title="Permalink"></a></h3><ul><li>TensorFlow with explicit graph vs. PyTorch with tape / Wengert list</li></ul><h3 id="How-to-test"><a class="docs-heading-anchor" href="#How-to-test">How to test</a><a id="How-to-test-1"></a><a class="docs-heading-anchor-permalink" href="#How-to-test" title="Permalink"></a></h3><p>A tricky example with odd function</p><h3 id="ChainRules"><a class="docs-heading-anchor" href="#ChainRules">ChainRules</a><a id="ChainRules-1"></a><a class="docs-heading-anchor-permalink" href="#ChainRules" title="Permalink"></a></h3><ul><li>Why you want to have it</li><li>Syntax</li><li>Structural vs Natural gradient</li></ul><h2 id="Why-custom-rules"><a class="docs-heading-anchor" href="#Why-custom-rules">Why custom rules</a><a id="Why-custom-rules-1"></a><a class="docs-heading-anchor-permalink" href="#Why-custom-rules" title="Permalink"></a></h2><ul><li>We need them for speed</li><li>They are needed for numerical stability</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../lecture_07/hw/">« Homework</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.10 on <span class="colophon-date" title="Friday 12 November 2021 06:33">Friday 12 November 2021</span>. Using Julia version 1.6.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
